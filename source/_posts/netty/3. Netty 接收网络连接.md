---
title: 3-Netty接收网络连接
tags:
  - Netty
  - 源码解析
categories:
  - Netty
abbrlink: 17349
date: 2025-02-12 20:00:22
---


## 相关链接

- [Netty是如何高效接收网络连接的](https://zhuanlan.zhihu.com/p/466019443)
- [ByteBuffer动态自适应扩缩容机制](https://zhuanlan.zhihu.com/p/471122992)

## Main Reactor 处理 OP_ACCEPT 事件

- Netty 将`OP_ACCEPT事件`处理的入口函数封装在`NioServerSocketChannel`里的底层操作类 Unsafe 的`read`方法中 --> `AbstractNioMessageChannel`

> io.netty.channel.nio.AbstractNioMessageChannel.NioMessageUnsafe#read

- 接收连接过程如下

![netty](/images/netty/03_01.PNG)

- **`MaxMessageHandle#maxMessagePerRead`**： 用于控制每次 read loop 里最大可以循环读取的次数，默认为 16 次，可在启动配置类`ServerBootstrap`中通过`ChannelOption.MAX_MESSAGES_PER_READ`选项设置。
- 客户端`NioSocketChannel`继承的是`AbstractNioByteChannel`，而服务端`NioServerSocketChannel`继承的是`AbstractNioMessageChannel`

> 客户端`NioSocketChannel`主要处理的是服务端与客户端的通信，这里涉及到接收客户端发送来的数据，而`Sub Reactor线程`从`NioSocketChannel`中读取的正是网络通信数据单位为`Byte`。 服务端`NioServerSocketChannel`主要负责处理`OP_ACCEPT事件`，创建用于通信的客户端`NioSocketChannel`。这时候客户端与服务端还没开始通信，所以`Main Reactor线程`从`NioServerSocketChannel`的读取对象为`Message`。这里的`Message`指的就是底层的`SocketChannel`客户端连接。

![netty](/images/netty/03_02.PNG)


## 接收网络数据总览

![netty](/images/netty/03_03.PNG)

- Netty 服务端对于一次 OP_READ 事件的处理，会在一个`do{}while()`循环 read loop 中分多次从客户端 NioSocketChannel 中读取网络数据。每次读取我们分配的 ByteBuffer 容量大小，初始容量为 2048

## ChannelRead 与 ChannelReadComplete 事件的区别

- `ChanneRead事件`：一次循环读取一次数据，就触发一次`ChannelRead事件`。本次最多读取在 read loop 循环开始分配的 DirectByteBuffer 容量大小。这个容量会动态调整。
- `ChannelReadComplete事件`：当读取不到数据或者不满足`continueReading`的任意一个条件就会退出 read loop，这时就会触发`ChannelReadComplete事件`。表示本次`OP_READ事件`处理完毕。

> 这里需要特别注意下触发`ChannelReadComplete事件`并不代表 NioSocketChannel 中的数据已经读取完了，只能说明本次`OP_READ事件`处理完毕。因为有可能是客户端发送的数据太多，Netty 读了`16次`还没读完，那就只能等到下次`OP_READ事件`到来的时候在进行读取了。

## AdaptiveRecvByteBufAllocator

AdaptiveRecvByteBufAllocator 主要的作用就是为接收数据的`ByteBuffer`进行扩容缩容，那么每次怎么扩容？扩容多少？怎么缩容？缩容多少呢？？

### 容量索引表

> Netty 中定义了一个`int型`的数组`SIZE_TABLE`来存储每个扩容单位对应的容量大小。建立起扩缩容的容量索引表。每次扩容多少，缩容多少全部记录在这个容量索引表中

- 在 AdaptiveRecvByteBufAllocatorl 类初始化的时候会在`static{}`静态代码块中对扩缩容索引表`SIZE_TABLE`进行初始化
- 当索引容量小于`512`时，`SIZE_TABLE`中定义的容量索引是从`16开始`按`16`递增

![netty](/images/netty/03_04.PNG)

- 当索引容量大于`512`时，`SIZE_TABLE`中定义的容量索引是按前一个索引容量的 2 倍递增

![netty](/images/netty/03_05.PNG)

### 扩缩容逻辑

> AdaptiveRecvByteBufAllocator 类中定义的扩容步长`INDEX_INCREMENT = 4`，缩容步长`INDEX_DECREMENT = 1`

假设当前`ByteBuffer`的容量索引为`33`，对应的容量为`2048`

- 扩容

当对容量为`2048`的 ByteBuffer 进行扩容时，根据当前的容量索引`index = 33` 加上 扩容步长`INDEX_INCREMENT = 4`计算出扩容后的容量索引为`37`，那么扩缩容索引表`SIZE_TABLE`下标`37`对应的容量就是本次 ByteBuffer 扩容后的容量`SIZE_TABLE[37] = 32768`

- 缩容

同理对容量为`2048`的 ByteBuffer 进行缩容时，我们就需要用当前容量索引`index = 33` 减去 缩容步长`INDEX_DECREMENT = 1`计算出缩容后的容量索引`32`，那么扩缩容索引表`SIZE_TABLE`下标`32`对应的容量就是本次 ByteBuffer 缩容后的容量`SIZE_TABLE[32] = 1024`

- 扩缩容时机
  - 每轮 read loop 结束之后，我们都会调用`allocHandle.readComplete()`来根据在 allocHandle 中统计的在本轮 read loop 中读取字节总大小，来决定在下一轮 read loop 中是否对 DirectByteBuffer 进行扩容或者缩容

 
![netty](/images/netty/03_06.png)

- `DEFAULT_INITIAL`： 表示 ByteBuffer 的初始化容量。默认为`2048`。对应的 index=33
- `DEFAULT_MINIMUM`： 表示 ByteBuffer 最小的容量，默认为`64`，也就是无论 ByteBuffer 在怎么缩容，容量也不会低于`64`，对应的 index=3
- `DEFAULT_MAXIMUM`： 表示 ByteBuffer 的最大容量，默认为`65536`，也就是无论 ByteBuffer 在怎么扩容，容量也不会超过`65536`，对应的 index=38
- 如果本次`OP_READ事件`实际读取到的总字节数`actualReadBytes`在 SIZE_TABLE[index - INDEX_DECREMENT]与 SIZE_TABLE[index]之间的话，也就是如果本轮 read loop 结束之后总共读取的字节数在`[1024,2048]`之间。说明此时分配的`ByteBuffer`容量正好，不需要进行缩容也不需要进行扩容。 比如本次`actualReadBytes = 2000`，正好处在`1024`与`2048`之间。说明`2048`的容量正好
- 如果`actualReadBytes` 小于等于 SIZE_TABLE[index - INDEX_DECREMENT]，也就是如果本轮 read loop 结束之后总共读取的字节数小于等于`1024`。表示本次读取到的字节数比当前 ByteBuffer 容量的下一级容量还要小，说明当前 ByteBuffer 的容量分配的有些大了，设置缩容标识`decreaseNow = true`。当下次`OP_READ事件`继续满足缩容条件的时候，开始真正的进行缩容。缩容后的容量为 SIZE_TABLE[index - INDEX_DECREMENT]，但不能小于 SIZE_TABLE[minIndex]

> 注意需要满足两次缩容条件才会进行缩容，且缩容步长为 1，缩容比较谨慎

- 如果本次`OP_READ事件`处理总共读取的字节数`actualReadBytes` 大于等于 当前 ByteBuffer 容量（nextReceiveBufferSize）时，说明 ByteBuffer 分配的容量有点小了，需要进行扩容。扩容后的容量为 SIZE_TABLE[index + INDEX_INCREMENT]，但不能超过 SIZE_TABLE[maxIndex]。

> 满足一次扩容条件就进行扩容，并且扩容步长为 4， 扩容比较奔放

## PooledByteBufAllocator

> `AdaptiveRecvByteBufAllocator`类只是负责动态调整 ByteBuffer 的容量，而具体为 ByteBuffer 申请内存空间的是由`PooledByteBufAllocator`负责

- 当数据达到网卡时，网卡会通过 DMA 的方式将数据拷贝到内核空间中，这是`第一次拷贝`。当用户线程在用户空间发起系统 IO 调用时，CPU 会将内核空间的数据再次拷贝到用户空间。这是`第二次拷贝`。于此不同的是当我们在 JVM 中发起 IO 调用时，比如我们使用 JVM 堆内存读取`Socket接收缓冲区`中的数据时，会多一次内存拷贝，CPU 在`第二次拷贝`中将数据从内核空间拷贝到用户空间时，此时的用户空间站在 JVM 角度是`堆外内存`，所以还需要将堆外内存中的数据拷贝到`堆内内存`中。这就是`第三次内存拷贝`。同理当我们在 JVM 中发起 IO 调用向`Socket发送缓冲区`写入数据时，JVM 会将 IO 数据先`拷贝`到`堆外内存`，然后才能发起系统 IO 调用。
- Netty 在进行 I/O 操作时都是使用的堆外内存，采用堆外内存为 ByteBuffer 分配内存的好处：
  - 堆外内存直接受操作系统的管理，不会受 JVM 的管理，所以 JVM 垃圾回收对应用程序的性能影响就没有了
  - 网络数据到达之后直接在`堆外内存`上接收，进程读取网络数据时直接在堆外内存中读取，所以就避免了`第三次内存拷贝`
- 由于堆外内存不受 JVM 的管理，所以就需要额外关注对内存的使用和释放，稍有不慎就会造成内存泄露，于是 Netty 就引入了内存池对`堆外内存`进行统一管理
- PooledByteBufAllocator 类的这个前缀`Pooled`就是`内存池`的意思，这个类会使用 Netty 的内存池为 ByteBuffer 分配`堆外内存`
- io.netty.channel.DefaultChannelConfig#allocator -> 

![netty](/images/netty/03_07.png)

![netty](/images/netty/03_08.png)
